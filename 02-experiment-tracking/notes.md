# Notes on Experiment Tracking

MLFlow helps organize machine learning projects keeping info about model performance, metrics, and history of one's ML project.

### Terminology
- ML experiment: the process of building a ML model (all the trial error/deployment of a model). The whole process of creating a model
- Experiment run: each trial/test in an experiment to creating the ML experiment. For example, each run of a new model would be an experiment run.
- Run artificact: any file associated with an ML run
- Experiment metadata: 



### MLFlow Experiment Tracking
Experiment tracking is process of keeping track of all relevant info:
- Source code
- Environment
- Data
- Model
- Hyperparameters

### Tracking experiments with MLFlow
MLFlow organizes experiments into runs and keeps track of in each run:
- Parameters
    - Preprocessing steps
    - Path to the training data
    - Hyperparameters
- Metrics
- Metadata
- Artifacts
- Models & their performance

MLFlow alos logs extra info about each run
- Source code
- Version of the code (git commit)
- Start and end time
- Author

# Getting started with MLFlow

One should probably create a conda environment to store mlflow.

There are two ways to run mlflow
1. `mlflow ui` starts mlflow gui
2. `mlflow ui --backend-store-uri sqlite:///mlflow.db` which tells mlflow that we want to store our metadata and artifacts in a sqllite server.
    - Then graph the url generated by the command and open it in a web browser

# Creating/starting MLFlow experiment
1. start by opening conda environment and typing `mlflow ui --backend-store-uri sqlite:///mlflow.db`
    - This will open up the mlflow ui and a sqlite server will store our info
2. Open the notebook in which we were creating out experiment and add to a cell this set of commands:

```python
mlflow.set_tracking_uri('sqlite:///mlflow.db')
mlflow.set_experiment('nyc-taxi-experiment')
```
    - The first line makes sure that mlflow knows where to store out model outputs and info
    - The second command says create an nyc-taxi-experiment ML experiment. If it already exists we just add the metadata at this location. If it doesn't exist we create a new one.
__Note: MAKE SURE YOU CREATE THE DATABASE AND RUN THE COMMAND FOR THE WORKFLOWS IN THE SAME FOLDER OTHERWISE THE CHANGES WILL NOT SHOW IN MLFLOW DUE TO THE PROJECT NOT BEING TRACKED!__
3. One can add `with mlflow.start_run() <code here...>` to a cell with the model one wants to train. MLFlow will then track the performance of that run. One can also add tags to the run with `mlflow.set_tag(<metadata>)` such as developer name, date, class of model used, etc.
- Here are some useful tags to add to a mlflow run:
    - mlflow.set_tag("developer", 'dan r')
    - mlflow.log_param("train-data-path", "green_tripdata_2021-01.parquet")
    - mlflow.log_param("train-data-path", "green_tripdata_2021-01.parquet")
    - mlflow.log_param("alpha", alpha)
    - mlflow.log_metric("rmse", rmse)

# Hyperparameter tracking in MLFlow

Use the library `hyperopt` which helps a model find the best parameters.

For using `hyperopt` with MLFlow we define an objective() function which handles the model training and logs key info.
We also need to define a `search_space` dictionary which gives the ranges that hyperopt can explore to optimize the parameter.

# Model Management

Model management consists of three different components:
- Experiment tracking (Will use MLFlow to track the experiment)
- Model versioning
- Model deployment
- Scaling hardware

One should avoid using directories to organize models as it is error prone, with inconsistent naming, and lots of other problems


To save different version of models there are two primary ways:
1.  use `mlflow.log_artifact(<path_to_model>, <path_to_where_mlflow_will_save_model)`
    - `mlflow.log_artifact(local_path='models/ling_reg.bin', artifact_path="models_pickle/")`
2. The other way is to mlflow.<model_framework>.log_model(<function_storing_model>, artifact_path=<fill_in_path>)
The pattern mlflow.<model_framework>.log_model() refers to different MLflow integrations with various ML frameworks. MLflow provides built-in support for many popular frameworks including:
- mlflow.xgboost
- mlflow.sklearn
- mlflow.pytorch
- mlflow.tensorflow
- mlflow.keras
- And many others
    - `mlflow.xgboost.log_model(booster, artifact_path='models_mlflow')` at the end of the xgboost will log the model. I think we need to define an objective function to do this
Note:
Each of these integrations knows how to properly serialize and save models from their respective frameworks. You need to:

Import the appropriate framework (e.g., import xgboost as xgb)
Create and train a model using that framework
Use the corresponding MLflow module to log the model

If we have saved a model using __method 2__ then we get a whole set of files:
- the `MLmodel` file is the most important. Stores key info about the model like where it lives 
- the `conda.yaml` file has the environment the model was used to run so we can reproduce the run
- the `requirements.txt` file is similar file as the `conda.yaml`

__NOTE: ONE SHOULD ALSO SAVE THE PREPROCESSOR USED TO CREATE THE MODEL. THIS MEANS WE WANT THE DICTVECTORIZER USED TO GENERATE THE MODEL. DONT FORGET TO INCUDE IT AS AN ARTIFACT!__
Logging the preprocessor should look something like: `mlflow.log_artifact(<local_path_to_store>, <path_to_store_on_MLFlow>)`
- `mlflow.log_artifact(../../preprocessors/preprocessor.bin, artifact_path='preprocessors')`
